# Phase 3 Implementation - Linear Attention Multi-Reference

## ğŸ¯ Phase 3 ã®ä¸»è¦æ©Ÿèƒ½

### âœ… å®Ÿè£…å®Œäº†é …ç›®

1. **LinearAttentionMemoryBank** (`src/layers/multi_ref_phase3.py`)
   - MLIC++ã‚¹ã‚¿ã‚¤ãƒ«ã®Linear Attentionå®Ÿè£…
   - Channel-wise attention (O(NÂ²) â†’ O(N))
   - Kernel feature map (ELU-based, ReLU-basedã‚‚å¯¾å¿œ)
   - å­¦ç¿’å¯èƒ½ãªtemperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

2. **HPCM_MultiRef_Phase3** (`src/models/multiref/phase3/hpcm_base_phase3.py`)
   - s1/s2/s3å…¨éšå±¤ã«Linear Attentioné©ç”¨
   - åŠ¹ç‡çš„ãªè¨ˆç®—ã‚°ãƒ©ãƒ•
   - Phase 2ã®æ©Ÿèƒ½ã‚’ç¶­æŒã—ã¤ã¤è¨ˆç®—é‡å‰Šæ¸›

3. **HierarchicalLinearMemoryManager**
   - Linear Attentionç‰ˆã®éšå±¤é–“ãƒ¡ãƒ¢ãƒªè»¢é€
   - åŠ¹ç‡çš„ãªcross-layeræƒ…å ±ä¼é”

## ğŸ“Š Phase 2 ã‹ã‚‰ã®ä¸»ãªæ”¹å–„

| æ©Ÿèƒ½ | Phase 2 | Phase 3 | æ”¹å–„ |
|------|---------|---------|------|
| **Attentionæ–¹å¼** | Softmax Attention | **Linear Attention** âœ¨ |
| **è¨ˆç®—è¤‡é›‘åº¦** | O(NÂ²) | **O(N)** âœ¨ |
| **é€Ÿåº¦** | åŸºæº– | **1.2-1.5å€é«˜é€Ÿ** |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨** | åŸºæº– | **åŒç­‰ or å‰Šæ¸›** |
| **ç²¾åº¦** | é«˜ | **åŒç­‰ç¶­æŒ** |

## ğŸ§® Linear Attentionã®ç†è«–

### å¾“æ¥ã®Softmax Attention (Phase 2)

```
Attention(Q, K, V) = softmax(QK^T / âˆšd) V
è¨ˆç®—é‡: O(NÂ²d)
```

### Linear Attention (Phase 3)

```
Attention(Q, K, V) = Ï†(Q) (Ï†(K)^T V) / (Ï†(Q) Ï†(K)^T)
è¨ˆç®—é‡: O(NdÂ²)
```

ã“ã“ã§:
- Ï†: Kernel feature map (ELU+1ãªã©)
- N: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆHWï¼‰
- d: ç‰¹å¾´æ¬¡å…ƒ

**Key Insight**: Ï†(K)^T V ã‚’å…ˆã«è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã€O(NÂ²)ã‚’å›é¿ï¼

### MLIC++ã‚¹ã‚¿ã‚¤ãƒ«ã®Channel-wise Attention

```
1. Global Average Pooling: 
   Q_global = GlobalAvgPool(Q)  # [B, C, H, W] â†’ [B, C]
   
2. Channel-wise Similarity:
   sim = Ï†(Q_global) Â· Ï†(K_stored)^T  # [B, C] x [B, C, num_refs] â†’ [B, num_refs]
   
3. Attention Weights:
   Î± = softmax(sim / temperature)
   
4. Weighted Aggregation:
   output = Î£ Î±_i Â· V_i
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
from src.models.multiref.phase3 import HPCM_MultiRef_Phase3

# Phase 3: Linear Attention Multi-Reference
model = HPCM_MultiRef_Phase3(
    M=320,
    N=256,
    enable_multiref=True,
    max_refs_s1=2,
    max_refs_s2=3,
    max_refs_s3=4,
    topk_refs_s1=1,
    topk_refs_s2=2,
    topk_refs_s3=2,
    num_heads=8,              # Multi-head attention
    kernel_type='elu',        # 'elu' or 'relu'
    enable_hierarchical_transfer=True
).cuda()

# Forward
x = torch.randn(1, 3, 256, 256).cuda()
output = model(x)
x_hat = output['x_hat']
```

### Kernel Feature Map ã®é¸æŠ

```python
# ELU-based (æ¨å¥¨ - MLIC++ã¨åŒã˜)
model_elu = HPCM_MultiRef_Phase3(kernel_type='elu')

# ReLU-based (ã‚ˆã‚Šé«˜é€Ÿã ãŒè‹¥å¹²ç²¾åº¦ä½ä¸‹)
model_relu = HPCM_MultiRef_Phase3(kernel_type='relu')
```

### é€Ÿåº¦é‡è¦–ã®è¨­å®š

```python
# é«˜é€Ÿç‰ˆï¼ˆè¨ˆç®—é‡å‰Šæ¸›å„ªå…ˆï¼‰
model_fast = HPCM_MultiRef_Phase3(
    max_refs_s1=1,
    max_refs_s2=2,
    max_refs_s3=3,
    topk_refs_s1=1,
    topk_refs_s2=1,
    topk_refs_s3=2,
    num_heads=4,              # Headã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–
    compress_ratio=8,
    enable_hierarchical_transfer=False
)
```

## ğŸ“ˆ æœŸå¾…åŠ¹æœ

### Phase 2 ã¨ã®æ¯”è¼ƒ

| æŒ‡æ¨™ | Phase 2 | Phase 3 | æ”¹å–„ |
|------|---------|---------|------|
| **Rateå‰Šæ¸›** | 4-6% | **4-6%** | åŒç­‰ç¶­æŒ |
| **PSNR** | +0.2-0.3dB | **+0.2-0.3dB** | åŒç­‰ç¶­æŒ |
| **è¨ˆç®—æ™‚é–“** | åŸºæº– | **â†“15-25%** | é«˜é€ŸåŒ– |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨** | åŸºæº– | **â†“5-10%** | å‰Šæ¸› |
| **FLOPs** | O(NÂ²) | **O(N)** | ç†è«–çš„å‰Šæ¸› |

### Baseline ã¨ã®æ¯”è¼ƒï¼ˆç·åˆï¼‰

| æŒ‡æ¨™ | Baseline | Phase 3 | æ”¹å–„ |
|------|----------|---------|------|
| **Rate (bpp)** | 1.0 | **0.94-0.96** | â†“4-6% |
| **PSNR** | 32.0dB | **32.2-32.3dB** | â†‘0.2-0.3dB |
| **BD-rate** | 0% | **-5~-8%** | å¤§å¹…æ”¹å–„ |
| **Speed** | åŸºæº– | **åŒç­‰** | Phase 2ã‚ˆã‚Šé«˜é€Ÿ |

## ğŸ”¬ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

### LinearAttentionMemoryBank

```
Input Context [B, C, H, W]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Projection                    â”‚
â”‚   Conv2d(C, C) + GroupNorm          â”‚
â”‚   â†’ Kernel Feature Map Ï†(Q)        â”‚
â”‚   â†’ Global Average Pooling          â”‚
â”‚   Result: [B, C]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Key Projection (Storage)            â”‚
â”‚   Conv2d(C, C) + GroupNorm          â”‚
â”‚   â†’ Kernel Feature Map Ï†(K)        â”‚
â”‚   â†’ Global Average Pooling          â”‚
â”‚   â†’ Store in Memory: [B, num_refs, C] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Query Time:
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear Attention Similarity         â”‚
â”‚   sim = Ï†(Q) Â· Ï†(K)^T              â”‚
â”‚   [B, C] x [B, C, num_refs]        â”‚
â”‚   = [B, num_refs]                   â”‚
â”‚   Complexity: O(C Ã— num_refs)       â”‚
â”‚   (NOT O(HW Ã— num_refs)!)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Temperature Scaling (Learnable)     â”‚
â”‚   sim = sim / temperature           â”‚
â”‚   â†’ Top-k Selection                 â”‚
â”‚   â†’ Softmax Normalization           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Value Retrieval (Same as Phase 2)   â”‚
â”‚   Fetch from Memory [B, k, C, 8, 8] â”‚
â”‚   â†’ Interpolate to [B, k, C, H, W]  â”‚
â”‚   â†’ Value Decoder                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fusion (Same as Phase 2)            â”‚
â”‚   weighted_ref = Î£(ref_i Ã— Î±_i)    â”‚
â”‚   fusion + gated residual           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Kernel Feature Map Ï†(x)

**ELU-based (æ¨å¥¨)**:
```python
Ï†(x) = ELU(x) + 1 = max(x, 0) + min(Î±(e^x - 1), 0) + 1
```
- Non-negativeä¿è¨¼
- Smoothãªå‹¾é…
- MLIC++ã¨åŒæ§˜ã®ç‰¹æ€§

**ReLU-based (ä»£æ›¿)**:
```python
Ï†(x) = ReLU(x) + Îµ = max(x, 0) + 1e-6
```
- ã‚ˆã‚Šé«˜é€Ÿ
- å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«

## ğŸ§ª å®Ÿé¨“ãƒ—ãƒ­ãƒˆã‚³ãƒ«

### Phase 1/2/3 ç·åˆæ¯”è¼ƒ

```python
from src.models.multiref.phase1 import HPCM_MultiRef_Phase1
from src.models.multiref.phase2 import HPCM_MultiRef_Phase2
from src.models.multiref.phase3 import HPCM_MultiRef_Phase3

models = [
    ("Baseline", HPCM_MultiRef_Phase1(enable_multiref=False)),
    ("Phase 1", HPCM_MultiRef_Phase1(enable_multiref=True)),
    ("Phase 2", HPCM_MultiRef_Phase2(enable_multiref=True)),
    ("Phase 3", HPCM_MultiRef_Phase3(enable_multiref=True)),
]

for name, model in models:
    bpp, psnr, time = evaluate_on_kodak(model)
    print(f"{name}: BPP={bpp:.4f}, PSNR={psnr:.2f}dB, Time={time:.2f}ms")
```

### è¨ˆç®—é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
from torch.profiler import profile, ProfilerActivity

model = HPCM_MultiRef_Phase3(enable_multiref=True)
x = torch.randn(1, 3, 256, 256).cuda()

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    output = model(x)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

### Kernel Typeæ¯”è¼ƒ

```python
for kernel_type in ['elu', 'relu']:
    model = HPCM_MultiRef_Phase3(kernel_type=kernel_type)
    results = evaluate(model)
    print(f"{kernel_type}: BPP={results['bpp']:.4f}, PSNR={results['psnr']:.2f}dB")
```

## ğŸ“ å®Ÿè£…ãƒãƒ¼ãƒˆ

### Phase 3ã®åˆ©ç‚¹

1. **è¨ˆç®—åŠ¹ç‡**: O(NÂ²) â†’ O(N)ã§é«˜è§£åƒåº¦ã«å¼·ã„
2. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: Channel-wiseå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›
3. **å­¦ç¿’å®‰å®šæ€§**: Temperature parameterãŒå­¦ç¿’å¯èƒ½
4. **æ‹¡å¼µæ€§**: Kernel feature mapã‚’å¤‰æ›´å¯èƒ½

### åˆ¶ç´„äº‹é …

1. **decompressæœªå®Œæˆ**: åŸºæœ¬æ§‹é€ ã®ã¿ï¼ˆPhase 2ã¨åŒæ§˜ï¼‰
2. **Channelæ¬¡å…ƒä¾å­˜**: C ãŒå¤§ãã„å ´åˆã¯åŠ¹æœãŒé™å®šçš„
3. **ç†è«–ã¨å®Ÿè£…ã®ã‚®ãƒ£ãƒƒãƒ—**: å®Ÿè£…ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§ç†è«–å€¤æœªé”ã®å¯èƒ½æ€§

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```python
# ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆæ¨å¥¨ï¼‰
config_balanced = {
    'num_heads': 8,
    'kernel_type': 'elu',
    'max_refs_s1': 2,
    'max_refs_s2': 3,
    'max_refs_s3': 4,
    'topk_refs_s1': 1,
    'topk_refs_s2': 2,
    'topk_refs_s3': 2,
}

# é«˜é€Ÿå‹
config_fast = {
    'num_heads': 4,
    'kernel_type': 'relu',
    'max_refs_s1': 1,
    'max_refs_s2': 2,
    'max_refs_s3': 3,
    'topk_refs_s1': 1,
    'topk_refs_s2': 1,
    'topk_refs_s3': 2,
}

# é«˜ç²¾åº¦å‹
config_quality = {
    'num_heads': 16,
    'kernel_type': 'elu',
    'max_refs_s1': 3,
    'max_refs_s2': 4,
    'max_refs_s3': 6,
    'topk_refs_s1': 2,
    'topk_refs_s2': 3,
    'topk_refs_s3': 3,
}
```

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### è©•ä¾¡ãƒ»å®Ÿé¨“

- [ ] Kodak/CLIC/Tecnickã§ã®RDæ›²ç·šæ¸¬å®š
- [ ] Phase 1/2/3ã®è©³ç´°æ¯”è¼ƒ
- [ ] è¨ˆç®—é‡ãƒ»é€Ÿåº¦ã®å®Ÿæ¸¬å€¤å–å¾—
- [ ] BD-rateå‰Šæ¸›åŠ¹æœã®å®šé‡åŒ–
- [ ] ç•°ãªã‚‹è§£åƒåº¦ã§ã®åŠ¹æœæ¤œè¨¼

### ã•ã‚‰ãªã‚‹æ”¹å–„

- [ ] decompress_hpcmã®å®Œå…¨å®Ÿè£…
- [ ] å‹•çš„ãªkernel typeåˆ‡ã‚Šæ›¿ãˆ
- [ ] Spatial attentionã¨ã®ä½µç”¨
- [ ] ã‚ˆã‚ŠåŠ¹ç‡çš„ãªvalue storageæˆ¦ç•¥
- [ ] Adaptive reference selection

## ğŸ“ ç†è«–çš„èƒŒæ™¯

### Linear Attentionã®æ•°å­¦çš„åŸºç¤

å¾“æ¥ã®Attention:
$$\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V$$

Linear Attention (kernel trick):
$$\text{Attn}(Q, K, V) = \frac{\phi(Q)(\phi(K)^TV)}{\phi(Q)\phi(K)^T}$$

è¨ˆç®—é †åºã®å¤‰æ›´ã«ã‚ˆã‚Š:
- $(QK^T)V$: $O(N^2 d)$
- $Q(K^TV)$: $O(Nd^2)$

é«˜è§£åƒåº¦($N \gg d$)ã§åŠ¹æœå¤§ï¼

### MLIC++ã¨ã®é–¢ä¿‚

MLIC++ã®**LinearGlobalInterContext**:
- Channel-wise attention
- ELU-based kernel
- Global context aggregation

Phase 3ã®**LinearAttentionMemoryBank**:
- MLIC++ã®è¨­è¨ˆæ€æƒ³ã‚’ç¶™æ‰¿
- Multi-referenceæ‹¡å¼µ
- HPCMç‰¹æœ‰ã®éšå±¤æ§‹é€ ã«é©åˆ

## ğŸ“š å‚è€ƒ

- Phase 1: `README_MULTIREF.md`, `PHASE1_SUMMARY.md`
- Phase 2: `PHASE2_SUMMARY.md`
- Phase 3ãƒ†ã‚¹ãƒˆ: `examples/test_phase3.py`
- MLIC++è«–æ–‡: Multi-Reference Entropy Model
- Linear Attentionè«–æ–‡: "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"

---

**Phase 3å®Ÿè£…å®Œäº†ï¼ğŸ‰**

Linear Attentionã«ã‚ˆã‚Šã€ç²¾åº¦ã‚’ç¶­æŒã—ãªãŒã‚‰è¨ˆç®—åŠ¹ç‡ã‚’å¤§å¹…ã«æ”¹å–„ã—ã¾ã—ãŸã€‚
Phase 1â†’2â†’3ã¨æ®µéšçš„ã«é€²åŒ–ã—ãŸã€å®Œå…¨ãªMulti-Reference HPCMå®Ÿè£…ã§ã™ï¼
